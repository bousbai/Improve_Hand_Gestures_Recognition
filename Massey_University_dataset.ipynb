{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Massey University dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvIWHB2ZAhXJ"
      },
      "source": [
        "!pip uninstall keras-nightly\n",
        "\n",
        "!pip uninstall -y tensorflow\n",
        "\n",
        "!pip install h5py==2.10.0  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APCtUGSEV4fB"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install keras==2.2.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyE_VU9JA07Y",
        "outputId": "8127a4b1-3dbb-4b32-cd55-6c89d7f0ddba"
      },
      "source": [
        "import os\n",
        "import keras\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import itertools\n",
        "from keras import callbacks\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from capsulelayers import Length, Mask,CapsuleLayer, PrimaryCap\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import TensorBoard\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import layers, models\n",
        "from keras import initializers, layers\n",
        "from __future__ import print_function\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, TimeDistributed\n",
        "from keras.layers import Activation, Dense, Input\n",
        "from keras.layers import LSTM\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Reshape, Conv2DTranspose\n",
        "from keras.utils import CustomObjectScope\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Dense, Dropout, GaussianNoise, Conv1D\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "from keras import layers, models, optimizers\n",
        "from keras import backend as K\n",
        "import cv2\n",
        "import itertools\n",
        "from sklearn import svm, datasets\n",
        "import sklearn.model_selection as model_selection\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import combine_images\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
        "from keras.layers import concatenate\n",
        "from tensorflow import keras\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AI/CapsNet-Keras\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DunCYBf4BA_u"
      },
      "source": [
        "train_path = \"/content/drive/MyDrive/..train_path\"\n",
        "test_path = \"/content/drive/MyDrive/..test_path\"\n",
        "label_path = ['A', 'B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEsRhotOBEal"
      },
      "source": [
        "classes = ['A', 'B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlrS6PkyBIt2"
      },
      "source": [
        "DATADIR = train_path\n",
        "\n",
        "CATEGORIES = classes\n",
        "\n",
        "for category in CATEGORIES: \n",
        "    path = os.path.join(DATADIR,category) \n",
        "    for img in os.listdir(path): \n",
        "        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "        plt.imshow(img_array, cmap='gray')  # graph it\n",
        "        plt.show()  # display!\n",
        "\n",
        "        break  # we just want one for now so break\n",
        "    break  #...and one more!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSCtVd_9Beb-"
      },
      "source": [
        "new_image =cv2.resize(img_array, (40, 40))\n",
        "plt.imshow(new_image, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_IkDeRNBhsp"
      },
      "source": [
        "IMG_SIZE=40\n",
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES: \n",
        "\n",
        "        path = os.path.join(DATADIR,category)  \n",
        "        class_num = CATEGORIES.index(category) \n",
        "        for img in tqdm(os.listdir(path)): \n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                training_data.append([new_array, class_num])  # add this to our training_data\n",
        "            except Exception as e:  # in the interest in keeping the output clean...\n",
        "                pass\n",
        "           \n",
        "create_training_data()\n",
        "\n",
        "print(len(training_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag3QumxnBxLI"
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaebIWB1B1QK"
      },
      "source": [
        "for sample in training_data[:10]:\n",
        "    print(sample[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYhduXHrB4pJ"
      },
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for features,label in training_data:\n",
        "    x_train.append(features)\n",
        "    y_train.append(label)\n",
        "\n",
        "print(x_train[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "x_train = np.array(x_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC67qfZwistp"
      },
      "source": [
        "pickle_out = open(\"x_train.pickle\",\"wb\")\n",
        "pickle.dump(x_train, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"y_train.pickle\",\"wb\")\n",
        "pickle.dump(y_train, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peXzHba9B-pk"
      },
      "source": [
        "DATADIR = test_path\n",
        "\n",
        "CATEGORIES = classes\n",
        "\n",
        "for category in CATEGORIES:  \n",
        "    path = os.path.join(DATADIR,category)  \n",
        "    for img in os.listdir(path): \n",
        "        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "        plt.imshow(img_array, cmap='gray')  # graph it\n",
        "        plt.show()  # display!\n",
        "\n",
        "        break  # we just want one for now so break\n",
        "    break  #...and one more!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMR91wfiCE34"
      },
      "source": [
        "IMG_SIZE=40\n",
        "testing_data = []\n",
        "\n",
        "def create_testing_data():\n",
        "    for category in CATEGORIES:  \n",
        "\n",
        "        path = os.path.join(DATADIR,category)  \n",
        "        class_num = CATEGORIES.index(category)  \n",
        "\n",
        "        for img in tqdm(os.listdir(path)): \n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  \n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) \n",
        "                testing_data.append([new_array, class_num]) \n",
        "            except Exception as e:  \n",
        "                pass\n",
        "\n",
        "\n",
        "create_testing_data()\n",
        "\n",
        "print(len(testing_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMnbd8yWCRWV"
      },
      "source": [
        "random.shuffle(testing_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04hyrdmaCUrx"
      },
      "source": [
        "for sample in testing_data[:10]:\n",
        "    print(sample[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4x0PIpsCXMU"
      },
      "source": [
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for features,label in testing_data: \n",
        "    for sample in testing_data[:10]:\n",
        "        print(sample[1])\n",
        "    x_test.append(features)\n",
        "    y_test.append(label)\n",
        "\n",
        "print(x_test[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "x_test = np.array(x_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUDOVrVMCZRM"
      },
      "source": [
        "pickle_out = open(\"x_test.pickle\",\"wb\")\n",
        "pickle.dump(x_test, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"y_test.pickle\",\"wb\")\n",
        "pickle.dump(y_test, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxdcLl8zrgC8"
      },
      "source": [
        "pickle_in = open(\"x_train.pickle\",\"rb\")\n",
        "x_train = pickle.load(pickle_in)/ 255.\n",
        "pickle_in = open(\"y_train.pickle\",\"rb\")\n",
        "y_train = pickle.load(pickle_in)\n",
        "pickle_in = open(\"x_test.pickle\",\"rb\")\n",
        "x_test = pickle.load(pickle_in)/ 255.\n",
        "pickle_in = open(\"y_test.pickle\",\"rb\")\n",
        "y_test = pickle.load(pickle_in)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "print('Training', x_train.shape, x_train.max())\n",
        "print('Testing', x_test.shape, x_test.max())\n",
        "print('Training', y_train.shape, y_train.max())\n",
        "print('Testing', y_test.shape, y_test.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DubNsAZFChUm"
      },
      "source": [
        "#capsNet_model \n",
        "def CapsNet(input_shape, n_class, num_routing):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 4d, [None, width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param num_routing: number of routing iterations\n",
        "    :return: A Keras Model with 2 inputs and 2 outputs\n",
        "    \"\"\"\n",
        "    x = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=32, kernel_size=10, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
        "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=16, kernel_size=9, strides=2, padding='valid')\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='out_caps')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    y = layers.Input(shape=(n_class,))\n",
        "    masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.\n",
        "    x_recon = layers.Dense(64, activation='relu')(masked)\n",
        "    x_recon = layers.Dense(128, activation='relu')(x_recon)\n",
        "    x_recon = layers.Dense(1600, activation='softmax')(x_recon)\n",
        "    x_recon = layers.Reshape(target_shape=input_shape, name='out_recon')(x_recon)\n",
        "\n",
        "    # two-input-two-output keras Model\n",
        "    return models.Model([x, y], [out_caps, x_recon])\n",
        "\"----------------------------------------------------------------------------------------------\"\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.25 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))\n",
        "\"------------------------------------------------------------------------------------------------\"\n",
        "model = CapsNet(input_shape=[40, 40, 1],\n",
        "                n_class=26,\n",
        "                num_routing=3)\n",
        "model.summary()\n",
        "try:\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "except Exception as e:\n",
        "    print('No fancy plot {}'.format(e))\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzSb-ELsCmNz"
      },
      "source": [
        "# unpacking the data\n",
        "    #(x_train, y_train), (x_test, y_test) = data\n",
        "\n",
        "    # callbacks\n",
        "log = callbacks.CSVLogger('log.csv')\n",
        "    #checkpoint = callbacks.ModelCheckpoint('weights-{epoch:02d}.h5',\n",
        "     #                                      save_best_only=True, save_weights_only=True, verbose=1)\n",
        "#tb = callbacks.TensorBoard(log_dir=\"/content/drive/My Drive/AI/CapsNet-Keras/logs/{}\",\n",
        " #                           batch_size=128, histogram_freq=1)\n",
        "lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
        "# compile the mode\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                  loss=[margin_loss, 'mse'],\n",
        "                  loss_weights=[1., 0.0392],\n",
        "                  metrics={'out_caps': 'accuracy'})\n",
        "                  \n",
        "                  \n",
        "    \n",
        "\n",
        "# Training without data augmentation:\n",
        "\"\"\"\n",
        "history=model.fit([x_train, y_train], [y_train, x_train], batch_size=128, epochs=13,\n",
        "              validation_data=[[x_test_d, y_test_d], [y_test_d, x_test_d]])\n",
        "    \n",
        "  \n",
        "\"\"\"\n",
        "\n",
        "   # -----------------------------------Begin: Training with data augmentation -----------------------------------#\n",
        "def train_generator(x, y, batch_size, shift_fraction):\n",
        "        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n",
        "                                           height_shift_range=shift_fraction,\n",
        "                                           rotation_range=15,\n",
        "                                           fill_mode='nearest')  # shift up to 2 pixel for MNIST\n",
        "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
        "        while 1:\n",
        "            x_batch, y_batch = generator.next()\n",
        "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
        "        \n",
        "      # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
        "history=model.fit_generator(generator=train_generator(x_train, y_train, 128, 0.2),\n",
        "                        steps_per_epoch=int(1*y_train.shape[0] /128),\n",
        "                        epochs=20,\n",
        "                        validation_data=[[x_test, y_test], [y_test, x_test]]\n",
        "                        )\n",
        "    # -----------------------------------End: Training with data augmentation -----------------------------------#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKpByrggAzBv",
        "outputId": "8e5de4df-fd2d-4b7a-c423-1ed740e7f34c"
      },
      "source": [
        "model.save('Caps_Massey.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQd5adzrC_WK"
      },
      "source": [
        "score = model.evaluate([x_test_d, y_test_d], [y_test_d, x_test_d], verbose=0)\n",
        "print('Test accuracy:', score[3])\n",
        "print('Test loss:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn2pBT_NDFKa"
      },
      "source": [
        "%%time\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "\n",
        "# Compute confusion matrix\n",
        "class_names = ['class 1(A)', 'class 2(B)', 'class 3(C)','class 4(D)','class 5(E)','class 6(F)','class 7(G)','class 8(H)','class 9(I)','class 10(J)','class 11(K)','class 12(L)','class 13(M)','class 14(N)','class 15(O)','class 16(P)','class 17(Q)','class 18(R)','class 19(S)','class 20(T)','class 21(U)','class 22(V)','class 23(W)','class 24(X)','class 25(Y)','class 26(Z)']\n",
        "y_pred,xrec = model.predict([x_test, y_test])\n",
        "y_pred=np.argmax(y_pred, 1)\n",
        "cnf_matrix = confusion_matrix(np.argmax([y_test, x_test][0],axis=1), y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "h=len(xrec)\n",
        "print(cnf_matrix.shape)\n",
        "class_names = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]\n",
        "plt.figure()\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(10, 7)\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='CapsNet_1, without normalization',\n",
        "                      cmap=\"Blues\")\n",
        "plt.savefig('CapsNet_1_confusion_matrix.png')\n",
        "target_names = ['class 1(A)', 'class 2(B)', 'class 3(C)','class 4(D)','class 5(E)','class 6(F)','class 7(G)','class 8(H)','class 9(I)','class 10(J)','class 11(K)','class 12(L)','class 13(M)','class 14(N)','class 15(O)','class 16(P)','class 17(Q)','class 18(R)','class 19(S)','class 20(T)','class 21(U)','class 22(V)','class 23(W)','class 24(X)','class 25(Y)','class 26(Z)']\n",
        "\n",
        "print(classification_report(np.argmax([y_test,x_test][0],axis=1), y_pred,target_names=target_names))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NI3DgUvD1Qv"
      },
      "source": [
        "#-----------------------------------------------CNN_Model------------------------------------------------------------------------------\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 16,kernel_size = (5,5),input_shape = (40,40,1),activation = 'relu',padding = 'same'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(filters = 32,kernel_size = (5,5),padding = 'same',activation = 'relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(32,kernel_size = (5,5),padding = 'same',activation = 'relu'))\n",
        "model.add(Dropout(0.75))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64,activation = 'relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(26,activation = 'softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqBIbpgKD9da"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCfbd1gRECdq"
      },
      "source": [
        "history=model.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=15,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test_d,y_test_d))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKqzYAVhfyh3"
      },
      "source": [
        "model.save('CNN_Massey.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbOut1S304IY"
      },
      "source": [
        "score = model.evaluate(x_test_d, y_test_d, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYvE0Dnq09G1"
      },
      "source": [
        "# with data augmentation:\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=0.0009), metrics=['accuracy'])\n",
        "\n",
        "gen = ImageDataGenerator(width_shift_range=0.2,\n",
        "                         height_shift_range=0.2,\n",
        "                         rotation_range=15,\n",
        "                         fill_mode='nearest')\n",
        "\n",
        "test_gen = ImageDataGenerator()\n",
        "\n",
        "train_generator = gen.flow(x_train, y_train, batch_size=128)\n",
        "test_generator = test_gen.flow(x_test_d, y_test_d, batch_size=128)\n",
        "\n",
        "history=model.fit_generator(train_generator, steps_per_epoch=1*y_train.shape[0]/128, epochs=15, \n",
        "                    validation_data=test_generator, validation_steps=1*test.shape[0]/128)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTzQi8qT0_j1",
        "outputId": "9bfe3ddf-ab63-4a34-92ef-5cdf36a1ed04"
      },
      "source": [
        "model.save('CNN_with_DA_Massey.h5') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB003GjnErjV"
      },
      "source": [
        "score = model.evaluate(x_test_d, y_test_d, verbose=10)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjRZw56REzsg"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "\n",
        "# Compute confusion matrix\n",
        "class_names = ['class 1(A)', 'class 2(B)', 'class 3(C)','class 4(D)','class 5(E)','class 6(F)','class 7(G)','class 8(H)','class 9(I)','class 11(K)','class 12(L)','class 13(M)','class 14(N)','class 15(O)','class 16(P)','class 17(Q)','class 18(R)','class 19(S)','class 20(T)','class 21(U)','class 22(V)','class 23(W)','class 24(X)','class 25(Y)']\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred=np.argmax(y_pred, 1)\n",
        "cnf_matrix = confusion_matrix(np.argmax(y_test,axis=1), y_pred)\n",
        "print('arg:',np.argmax(y_test,axis=1))\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "class_names = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\"]\n",
        "\n",
        "plt.figure()\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(10, 7)\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='CNN, without normalization',\n",
        "                      cmap=\"Blues\")\n",
        "target_names = ['class 1(A)', 'class 2(B)', 'class 3(C)','class 4(D)','class 5(E)','class 6(F)','class 7(G)','class 8(H)','class 9(I)','class 11(K)','class 12(L)','class 13(M)','class 14(N)','class 15(O)','class 16(P)','class 17(Q)','class 18(R)','class 19(S)','class 20(T)','class 21(U)','class 22(V)','class 23(W)','class 24(X)','class 25(Y)']\n",
        "print(classification_report(np.argmax([y_test][0],axis=1), y_pred,target_names=target_names))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URTfXWCcZ5Wo"
      },
      "source": [
        "custom_objects={'CapsuleLayer': CapsuleLayer,'Mask':Mask,'Length':Length,'margin_loss':margin_loss}\n",
        "model = load_model('Caps_with_DA_Massey.h5', custom_objects=custom_objects)\n",
        "score = model.evaluate([x_test, y_test], [y_test, x_test], verbose=0)\n",
        "print('Test accuracy:', score[3])\n",
        "print('Test loss:', score[1])\n",
        "layer_name = 'digitcaps'\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,\n",
        "                                 outputs=model.get_layer(layer_name).output)\n",
        "Z1_train_DA = intermediate_layer_model.predict([x_train, y_train])\n",
        "Z1_test_DA = intermediate_layer_model.predict([x_test, y_test])\n",
        "print(Z1_train_DA.shape)\n",
        "print(Z1_test_DA.shape)\n",
        "\n",
        "Z1_train_DA=Z1_train_DA.reshape(1415,416)\n",
        "Z1_test_DA=Z1_test_DA.reshape(364,416)\n",
        "print(Z1_train_DA.shape)\n",
        "print(Z1_test_DA.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiK8uFgUJV0u"
      },
      "source": [
        "model = load_model('CNN_with_DA_Massey.h5')  # the original model\n",
        "model.summary()\n",
        "score = model.evaluate(x_test_d, y_test_d, verbose=10)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "layer_name = 'flatten'\n",
        "intermediate_layer_model = Model(inputs=model.input,\n",
        "                                 outputs=model.get_layer(layer_name).output)\n",
        "Z2_train_DA = intermediate_layer_model.predict(x_train)\n",
        "Z2_test_DA = intermediate_layer_model.predict(x_test)\n",
        "print(Z2_train_DA.shape)\n",
        "print(Z2_test_DA.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkNqEwC1Z2Qq"
      },
      "source": [
        "Z_train=np.concatenate((Z1_train_DA, Z2_train_DA), axis=1)\n",
        "Z_test=np.concatenate((Z1_test_DA, Z2_test_DA), axis=1)\n",
        "print(Z_train.shape)\n",
        "print(Z_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGH_Zx6gw4z-"
      },
      "source": [
        "scaler = StandardScaler(N)# Z_train.shape=(N,M)\n",
        "scaler.fit(Z_train)\n",
        "X_sc_train = scaler.transform(Z_train)\n",
        "X_sc_test = scaler.transform(Z_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39sMP0_1w2rN"
      },
      "source": [
        "pca = PCA(n_components=N)# Z_train.shape=(N,M)\n",
        "pca.fit(Z_train)\n",
        "\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJiV-ENfw0xi"
      },
      "source": [
        "NCOMPONENTS = 500\n",
        "\n",
        "pca = PCA(n_components=NCOMPONENTS)\n",
        "X_pca_train = pca.fit_transform(X_sc_train)\n",
        "X_pca_test = pca.transform(X_sc_test)\n",
        "pca_std = np.std(X_pca_train)\n",
        "\n",
        "print(X_sc_train.shape)\n",
        "print(X_pca_train.shape)\n",
        "print(X_pca_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU2nsljxwwys"
      },
      "source": [
        "poly = svm.SVC(kernel='linear').fit(X_pca_train, np.argmax(y_train,axis=1))\n",
        "poly_pred = poly.predict(X_pca_test)\n",
        "\n",
        "poly_accuracy = accuracy_score(np.argmax(y_test_d,axis=1), poly_pred)\n",
        "poly_f1 = f1_score(np.argmax(y_test_d,axis=1), poly_pred, average='weighted')\n",
        "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
        "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX_DPNJwWjRE"
      },
      "source": [
        "%%time\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "\n",
        "# Compute confusion matrix\n",
        "class_names = ['class 1(A)', 'class 2(B)', 'class 3(C)','class 4(D)','class 5(E)','class 6(F)','class 7(G)','class 8(H)','class 9(I)','class 10(J)','class 11(K)','class 12(L)','class 13(M)','class 14(N)','class 15(O)','class 16(P)','class 17(Q)','class 18(R)','class 19(S)','class 20(T)','class 21(U)','class 22(V)','class 23(W)','class 24(X)','class 25(Y)','class 26(Z)']\n",
        "y_pred = poly.predict(X_pca_test)\n",
        "#y_pred=np.argmax(y_pred, 1)\n",
        "cnf_matrix = confusion_matrix(np.argmax(y_test_d,axis=1), y_pred)\n",
        "print('arg:',np.argmax(y_test_d,axis=1))\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "class_names = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]\n",
        "\n",
        "plt.figure()\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(10, 7)\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='CNN, without normalization',\n",
        "                      cmap=\"Blues\")\n",
        "target_names =['class 1(A)', 'class 2(B)', 'class 3(C)','class 4(D)','class 5(E)','class 6(F)','class 7(G)','class 8(H)','class 9(I)','class 10(J)','class 11(K)','class 12(L)','class 13(M)','class 14(N)','class 15(O)','class 16(P)','class 17(Q)','class 18(R)','class 19(S)','class 20(T)','class 21(U)','class 22(V)','class 23(W)','class 24(X)','class 25(Y)','class 26(Z)']\n",
        "print(classification_report(np.argmax([y_test_d][0],axis=1), y_pred,target_names=target_names))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}